{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to5PiSVs0aNU",
        "outputId": "b10d5107-7f6e-450c-c322-3e0e1260008f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find torchmetrics... installing it.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "Train size: 190\n",
            "Test size: 11\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math \n",
        "import numpy as np\n",
        "import numpy.random as random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data.dataloader import Dataset\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, ConcatDataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import KFold\n",
        "from PIL import Image\n",
        "import gc\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to get torchmetrics, install it if it doesn't work\n",
        "try:\n",
        "  import torchmetrics\n",
        "except:\n",
        "  print(\"[INFO] Couldn't find torchmetrics... installing it.\")\n",
        "  !pip install -q torchmetrics \n",
        "  import torchmetrics\n",
        "\n",
        "\n",
        "class CreateDataset():\n",
        "    '''\n",
        "    Takes a data directory (the location fo the training or testing folders) as input, as well as the \n",
        "    number of augments and the desired augmentation transform. Number of augments and the augmentation\n",
        "    are optional, set to 0 and None respectively.\n",
        "    Returns a dataset of PIL images.\n",
        "    '''\n",
        "    def __init__(self, data_dir, n_of_augments=0, aug_transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.dataset = self.pair_image_with_mask()\n",
        "        self.n_of_augments = n_of_augments\n",
        "        self.aug_transform = aug_transform\n",
        "        self.augment_img_and_msk()\n",
        "\n",
        "    def pair_image_with_mask(self):\n",
        "        filenames = os.listdir(self.data_dir)\n",
        "        filenames.sort()\n",
        "        self.dataset = []\n",
        "        self.size = len(filenames) // 2\n",
        "\n",
        "        for idx in range(self.size):\n",
        "            img_path_original = os.path.join(self.data_dir, filenames[idx*2])\n",
        "            labels_path_original = os.path.join(self.data_dir, filenames[idx*2 +1])\n",
        "            img_original = Image.open(img_path_original)\n",
        "            label_original  = Image.open(labels_path_original)\n",
        "            self.dataset.append((img_original, label_original))\n",
        "\n",
        "        return self.dataset\n",
        "\n",
        "    def augment_img_and_msk(self):\n",
        "\n",
        "        for i in range(self.n_of_augments):\n",
        "            base_pair = self.dataset[random.randint(0, self.size)] # Only augment original images to avoid heavy distortion\n",
        "            image = np.array(base_pair[0])\n",
        "            mask = np.array(base_pair[1])\n",
        "            aug_pair = self.aug_transform(image=image, mask=mask)\n",
        "            aug_image = aug_pair['image']\n",
        "            aug_mask = aug_pair['mask']\n",
        "        \n",
        "            # Convert back into PIL image\n",
        "            aug_image = Image.fromarray(aug_image.astype('uint8'))\n",
        "            aug_mask = Image.fromarray(aug_mask.astype('uint8'))\n",
        "            self.dataset.append((aug_image, aug_mask))\n",
        "\n",
        "        return self.dataset \n",
        "\n",
        "class MyDataset2(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    Required class for formatting a dataset and allowing the pytorch DataLoader to retreive images\n",
        "    in the correct form. Takes as input a dataset of PIL images and a transform to allow the images \n",
        "    to be of the right format to be used in subsequent models. Pairs the images with their masks and\n",
        "    turns them into tensors with shape [3, height, width].\n",
        "    '''\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.dataset[idx][0]\n",
        "        mask = self.dataset[idx][1]\n",
        "        \n",
        "        if self.transform:\n",
        "            image_np = np.array(img)\n",
        "            mask_np = np.array(mask)\n",
        "            aug = self.transform(image=image_np, mask=mask_np)\n",
        "            img=aug['image']\n",
        "            mask = aug['mask']\n",
        "            mask = np.transpose(mask, (2, 0, 1))\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "# Augmentation transform using Albumentations import\n",
        "aug_transform = A.Compose([\n",
        "                A.RandomCrop(width=960, height=720),\n",
        "                A.HorizontalFlip(p=0.8),\n",
        "                A.RandomBrightnessContrast(p=0.8),\n",
        "                A.RandomRotate90(p=0.8),\n",
        "                A.OneOf([\n",
        "                  A.CoarseDropout(max_holes=50,min_height=7,p=0.5),\n",
        "                  A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
        "                  A.GridDistortion(num_steps=4,p=0.5),\n",
        "                  A.HueSaturationValue(p=0.5)],\n",
        "                  p=1)\n",
        "                ])\n",
        "\n",
        "# Necessary transform for MyDataSet2, given the models used take images of input size 520x520\n",
        "transform = A.Compose([\n",
        "                  A.Resize(520, 520, p=1),\n",
        "                  ToTensorV2()\n",
        "                  ])\n",
        "\n",
        "# Define device to be cuda if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define file_name \n",
        "file_name = '/content/Cam101'\n",
        "\n",
        "# If file Cam101 is not saved locally, mount user's Google Drive and look for it there\n",
        "if not os.path.exists(file_name):\n",
        "      drive.mount('/content/drive')\n",
        "      file_name = '/content/drive/MyDrive/Cam101'\n",
        "    \n",
        "# Define paths for the train and test datasets\n",
        "train_path= os.path.join(file_name, 'train')\n",
        "test_path= os.path.join(file_name, 'test')\n",
        "\n",
        "# Initialise the train and test dataset with the desired number of augmented images\n",
        "train_dataset = CreateDataset(train_path, 100, aug_transform)\n",
        "train_set = train_dataset.dataset\n",
        "print(f\"Train size: {len(train_dataset.dataset)}\")\n",
        "\n",
        "test_dataset = CreateDataset(test_path)\n",
        "test_set = test_dataset.dataset\n",
        "print(f\"Test size: {len(test_dataset.dataset)}\")  \n",
        "\n",
        "# Initialise train and test dataloaders in order to pass into PyTorch's DataLoader wrapper\n",
        "trainLoader = MyDataset2(train_set, transform)\n",
        "testLoader = MyDataset2(test_set, transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h09sLE_y0emq"
      },
      "outputs": [],
      "source": [
        "# Use pandas to read the RGB-Class Name text file and create a classes dictionary\n",
        "classes_path = os.path.join(file_name, \"label_colors.txt\")\n",
        "df = pd.read_csv(classes_path, delim_whitespace=True, header=None)\n",
        "classes_dic = {}\n",
        "\n",
        "for idx, line in enumerate(df.values[:]):\n",
        "    classes_dic[idx] = tuple(int(rgb) for rgb in line[:-1])\n",
        "\n",
        "# Get the number of class names (one output unit for each class)\n",
        "output_shape = len(classes_dic.keys())\n",
        "\n",
        "#Create class tensor of shape [output_shape, 3] for encode_segmap()\n",
        "class_tensor = torch.tensor(list(classes_dic.values()), dtype=torch.uint8).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbHCKBYX0htT"
      },
      "outputs": [],
      "source": [
        "def encode_segmap(labels, class_tensor=class_tensor):\n",
        "    # Create a tensor of size [batch_size, height, width, 3]\n",
        "    labels = labels.permute(0, 2, 3, 1).to(device)\n",
        "    # Create a tensor of size [batch_size, height, width, num_classes] using broadcasting\n",
        "    one_hot = torch.all(labels[:, :, :, None, :] == class_tensor, dim=-1)\n",
        "    # Convert bool tensor to int tensor of size [batch_size, height, width, num_classes]\n",
        "    one_hot = one_hot.long()\n",
        "    # Convert tensor of size [batch_size, height, width, num_classes] to [batch_size, num_classes, height, width]\n",
        "    one_hot = one_hot.permute(0, 3, 1, 2)\n",
        "    return one_hot\n",
        "\n",
        "def one_hot_to_rgb(one_hot_tensor, classes_dic=classes_dic):\n",
        "    # Convert one-hot tensor to index array\n",
        "    one_hot_array = one_hot_tensor.cpu().numpy()\n",
        "    index_array = np.argmax(one_hot_array, axis=0)\n",
        "    \n",
        "    # Map index to RGB PIL image using classes_dic\n",
        "    color_array = np.take(np.array(list(classes_dic.values())), index_array, axis=0)\n",
        "    rgb_array = color_array.astype(np.uint8)\n",
        "    rgb_image = Image.fromarray(rgb_array, mode='RGB')\n",
        "    # rgb_image.to(device)\n",
        "    \n",
        "    return rgb_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb8XpYDq0jif"
      },
      "outputs": [],
      "source": [
        "#Garbage collect and empty cache\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "weight_dataset = CreateDataset(train_path) # Use the whole training dataset with no augmented images\n",
        "weight_set = weight_dataset.dataset\n",
        "weightLoader = MyDataset2(weight_set, transform)\n",
        "\n",
        "def weights_map(dataloader):\n",
        "    '''\n",
        "    Takes the (train) dataloader from which we select all masks and calculate the number of pixel \n",
        "    instances of each class.\n",
        "    '''\n",
        "    num_samples = len(weightLoader)\n",
        "    weight_dataloader = DataLoader(dataloader, batch_size=num_samples, shuffle=True)\n",
        "    images, masks = next(iter(weight_dataloader))\n",
        "    # Turn all masks into a one_hot tensor of shape [num_samples, 32, height, width]\n",
        "    one_hot_masks = encode_segmap(masks)\n",
        "    # Calculate an index tensor of shape [num_samples, height width] where each pixel has an integer value corresponding to the class index it belongs to\n",
        "    index_tensor = torch.argmax(one_hot_masks, dim=1)\n",
        "    # Create a tensor of length 32 that counts how many pixels belong to each class across the whole index tensor\n",
        "    classes_count = torch.zeros(output_shape)\n",
        "    for i in range(output_shape):\n",
        "        classes_count[i] = (index_tensor == i).sum().item()\n",
        "    \n",
        "    # Calculate the class weights according to the inverse of the frequencies\n",
        "    class_weights = torch.where(classes_count > 0,  (100*max(classes_count)/sum(classes_count).item()) - 50*(classes_count / sum(classes_count).item()), (100*max(classes_count)/sum(classes_count).item()))\n",
        "\n",
        "    return class_weights.to(device)\n",
        "\n",
        "class_weights = weights_map(weightLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZzuC79a0lcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8e4c71e-c73f-4a0b-dfee-d83cbd130a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet101_coco-586e9e4e.pth\n",
            "100%|██████████| 233M/233M [00:01<00:00, 207MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
            "/usr/local/lib/python3.10/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return super().__sizeof__() + self.nbytes()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================================================================================================\n",
            "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
            "==================================================================================================================================\n",
            "DeepLabV3 (DeepLabV3)                              [10, 3, 520, 520]    [10, 32, 520, 520]   --                   Partial\n",
            "├─IntermediateLayerGetter (backbone)               [10, 3, 520, 520]    [10, 2048, 65, 65]   --                   Partial\n",
            "│    └─Conv2d (conv1)                              [10, 3, 520, 520]    [10, 64, 260, 260]   (9,408)              False\n",
            "│    └─BatchNorm2d (bn1)                           [10, 64, 260, 260]   [10, 64, 260, 260]   (128)                False\n",
            "│    └─ReLU (relu)                                 [10, 64, 260, 260]   [10, 64, 260, 260]   --                   --\n",
            "│    └─MaxPool2d (maxpool)                         [10, 64, 260, 260]   [10, 64, 130, 130]   --                   --\n",
            "│    └─Sequential (layer1)                         [10, 64, 130, 130]   [10, 256, 130, 130]  --                   False\n",
            "│    │    └─Bottleneck (0)                         [10, 64, 130, 130]   [10, 256, 130, 130]  (75,008)             False\n",
            "│    │    └─Bottleneck (1)                         [10, 256, 130, 130]  [10, 256, 130, 130]  (70,400)             False\n",
            "│    │    └─Bottleneck (2)                         [10, 256, 130, 130]  [10, 256, 130, 130]  (70,400)             False\n",
            "│    └─Sequential (layer2)                         [10, 256, 130, 130]  [10, 512, 65, 65]    --                   False\n",
            "│    │    └─Bottleneck (0)                         [10, 256, 130, 130]  [10, 512, 65, 65]    (379,392)            False\n",
            "│    │    └─Bottleneck (1)                         [10, 512, 65, 65]    [10, 512, 65, 65]    (280,064)            False\n",
            "│    │    └─Bottleneck (2)                         [10, 512, 65, 65]    [10, 512, 65, 65]    (280,064)            False\n",
            "│    │    └─Bottleneck (3)                         [10, 512, 65, 65]    [10, 512, 65, 65]    (280,064)            False\n",
            "│    └─Sequential (layer3)                         [10, 512, 65, 65]    [10, 1024, 65, 65]   --                   True\n",
            "│    │    └─Bottleneck (0)                         [10, 512, 65, 65]    [10, 1024, 65, 65]   1,512,448            True\n",
            "│    │    └─Bottleneck (1)                         [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (2)                         [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (3)                         [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (4)                         [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (5)                         [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (6)                         [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (7)                         [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (8)                         [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (9)                         [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (10)                        [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (11)                        [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (12)                        [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (13)                        [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (14)                        [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (15)                        [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (16)                        [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (17)                        [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (18)                        [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (19)                        [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (20)                        [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (21)                        [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (22)                        [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    └─Sequential (layer4)                         [10, 1024, 65, 65]   [10, 2048, 65, 65]   --                   True\n",
            "│    │    └─Bottleneck (0)                         [10, 1024, 65, 65]   [10, 2048, 65, 65]   6,039,552            True\n",
            "│    │    └─Bottleneck (1)                         [10, 2048, 65, 65]   [10, 2048, 65, 65]   4,462,592            True\n",
            "│    │    └─Bottleneck (2)                         [10, 2048, 65, 65]   [10, 2048, 65, 65]   4,462,592            True\n",
            "├─DeepLabHead (classifier)                         [10, 2048, 65, 65]   [10, 32, 65, 65]     --                   True\n",
            "│    └─ASPP (0)                                    [10, 2048, 65, 65]   [10, 256, 65, 65]    --                   True\n",
            "│    │    └─ModuleList (convs)                     --                   --                   15,206,912           True\n",
            "│    │    └─Sequential (project)                   [10, 1280, 65, 65]   [10, 256, 65, 65]    328,192              True\n",
            "│    └─Conv2d (1)                                  [10, 256, 65, 65]    [10, 256, 65, 65]    589,824              True\n",
            "│    └─BatchNorm2d (2)                             [10, 256, 65, 65]    [10, 256, 65, 65]    512                  True\n",
            "│    └─ReLU (3)                                    [10, 256, 65, 65]    [10, 256, 65, 65]    --                   --\n",
            "│    └─Conv2d (4)                                  [10, 256, 65, 65]    [10, 32, 65, 65]     8,224                True\n",
            "├─FCNHead (aux_classifier)                         [10, 1024, 65, 65]   [10, 32, 65, 65]     --                   True\n",
            "│    └─Conv2d (0)                                  [10, 1024, 65, 65]   [10, 256, 65, 65]    2,359,296            True\n",
            "│    └─BatchNorm2d (1)                             [10, 256, 65, 65]    [10, 256, 65, 65]    512                  True\n",
            "│    └─ReLU (2)                                    [10, 256, 65, 65]    [10, 256, 65, 65]    --                   --\n",
            "│    └─Dropout (3)                                 [10, 256, 65, 65]    [10, 256, 65, 65]    --                   --\n",
            "│    └─Conv2d (4)                                  [10, 256, 65, 65]    [10, 32, 65, 65]     8,224                True\n",
            "│    └─Softmax (softmax)                           [10, 32, 65, 65]     [10, 32, 65, 65]     --                   --\n",
            "==================================================================================================================================\n",
            "Total params: 61,001,856\n",
            "Trainable params: 59,556,928\n",
            "Non-trainable params: 1,444,928\n",
            "Total mult-adds (T): 2.59\n",
            "==================================================================================================================================\n",
            "Input size (MB): 32.45\n",
            "Forward/backward pass size (MB): 40603.30\n",
            "Params size (MB): 244.01\n",
            "Estimated Total Size (MB): 40879.76\n",
            "==================================================================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fcn_resnet101_coco-7ecb50ca.pth\" to /root/.cache/torch/hub/checkpoints/fcn_resnet101_coco-7ecb50ca.pth\n",
            "100%|██████████| 208M/208M [00:00<00:00, 227MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================================================================================================================\n",
            "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
            "=============================================================================================================================\n",
            "FCN (FCN)                                     [10, 3, 520, 520]    [10, 32, 520, 520]   --                   Partial\n",
            "├─IntermediateLayerGetter (backbone)          [10, 3, 520, 520]    [10, 2048, 65, 65]   --                   Partial\n",
            "│    └─Conv2d (conv1)                         [10, 3, 520, 520]    [10, 64, 260, 260]   (9,408)              False\n",
            "│    └─BatchNorm2d (bn1)                      [10, 64, 260, 260]   [10, 64, 260, 260]   (128)                False\n",
            "│    └─ReLU (relu)                            [10, 64, 260, 260]   [10, 64, 260, 260]   --                   --\n",
            "│    └─MaxPool2d (maxpool)                    [10, 64, 260, 260]   [10, 64, 130, 130]   --                   --\n",
            "│    └─Sequential (layer1)                    [10, 64, 130, 130]   [10, 256, 130, 130]  --                   False\n",
            "│    │    └─Bottleneck (0)                    [10, 64, 130, 130]   [10, 256, 130, 130]  (75,008)             False\n",
            "│    │    └─Bottleneck (1)                    [10, 256, 130, 130]  [10, 256, 130, 130]  (70,400)             False\n",
            "│    │    └─Bottleneck (2)                    [10, 256, 130, 130]  [10, 256, 130, 130]  (70,400)             False\n",
            "│    └─Sequential (layer2)                    [10, 256, 130, 130]  [10, 512, 65, 65]    --                   False\n",
            "│    │    └─Bottleneck (0)                    [10, 256, 130, 130]  [10, 512, 65, 65]    (379,392)            False\n",
            "│    │    └─Bottleneck (1)                    [10, 512, 65, 65]    [10, 512, 65, 65]    (280,064)            False\n",
            "│    │    └─Bottleneck (2)                    [10, 512, 65, 65]    [10, 512, 65, 65]    (280,064)            False\n",
            "│    │    └─Bottleneck (3)                    [10, 512, 65, 65]    [10, 512, 65, 65]    (280,064)            False\n",
            "│    └─Sequential (layer3)                    [10, 512, 65, 65]    [10, 1024, 65, 65]   --                   True\n",
            "│    │    └─Bottleneck (0)                    [10, 512, 65, 65]    [10, 1024, 65, 65]   1,512,448            True\n",
            "│    │    └─Bottleneck (1)                    [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (2)                    [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (3)                    [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (4)                    [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (5)                    [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (6)                    [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (7)                    [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (8)                    [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (9)                    [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (10)                   [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (11)                   [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (12)                   [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (13)                   [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (14)                   [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (15)                   [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (16)                   [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (17)                   [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (18)                   [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (19)                   [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (20)                   [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (21)                   [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    │    └─Bottleneck (22)                   [10, 1024, 65, 65]   [10, 1024, 65, 65]   1,117,184            True\n",
            "│    └─Sequential (layer4)                    [10, 1024, 65, 65]   [10, 2048, 65, 65]   --                   True\n",
            "│    │    └─Bottleneck (0)                    [10, 1024, 65, 65]   [10, 2048, 65, 65]   6,039,552            True\n",
            "│    │    └─Bottleneck (1)                    [10, 2048, 65, 65]   [10, 2048, 65, 65]   4,462,592            True\n",
            "│    │    └─Bottleneck (2)                    [10, 2048, 65, 65]   [10, 2048, 65, 65]   4,462,592            True\n",
            "├─FCNHead (classifier)                        [10, 2048, 65, 65]   [10, 32, 65, 65]     --                   True\n",
            "│    └─Conv2d (0)                             [10, 2048, 65, 65]   [10, 512, 65, 65]    9,437,184            True\n",
            "│    └─BatchNorm2d (1)                        [10, 512, 65, 65]    [10, 512, 65, 65]    1,024                True\n",
            "│    └─ReLU (2)                               [10, 512, 65, 65]    [10, 512, 65, 65]    --                   --\n",
            "│    └─Dropout (3)                            [10, 512, 65, 65]    [10, 512, 65, 65]    --                   --\n",
            "│    └─Conv2d (4)                             [10, 512, 65, 65]    [10, 32, 65, 65]     16,416               True\n",
            "├─FCNHead (aux_classifier)                    [10, 1024, 65, 65]   [10, 32, 65, 65]     --                   True\n",
            "│    └─Conv2d (0)                             [10, 1024, 65, 65]   [10, 256, 65, 65]    2,359,296            True\n",
            "│    └─BatchNorm2d (1)                        [10, 256, 65, 65]    [10, 256, 65, 65]    512                  True\n",
            "│    └─ReLU (2)                               [10, 256, 65, 65]    [10, 256, 65, 65]    --                   --\n",
            "│    └─Dropout (3)                            [10, 256, 65, 65]    [10, 256, 65, 65]    --                   --\n",
            "│    └─Conv2d (4)                             [10, 256, 65, 65]    [10, 32, 65, 65]     8,224                True\n",
            "│    └─Softmax (softmax)                      [10, 32, 65, 65]     [10, 32, 65, 65]     --                   --\n",
            "=============================================================================================================================\n",
            "Total params: 54,322,816\n",
            "Trainable params: 52,877,888\n",
            "Non-trainable params: 1,444,928\n",
            "Total mult-adds (T): 2.33\n",
            "=============================================================================================================================\n",
            "Input size (MB): 32.45\n",
            "Forward/backward pass size (MB): 39911.04\n",
            "Params size (MB): 217.29\n",
            "Estimated Total Size (MB): 40160.78\n",
            "=============================================================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/lraspp_mobilenet_v3_large-d234d4ea.pth\" to /root/.cache/torch/hub/checkpoints/lraspp_mobilenet_v3_large-d234d4ea.pth\n",
            "100%|██████████| 12.5M/12.5M [00:00<00:00, 117MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================================================================\n",
            "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
            "============================================================================================================================================\n",
            "LRASPP (LRASPP)                                              [10, 3, 520, 520]    [10, 32, 520, 520]   --                   Partial\n",
            "├─IntermediateLayerGetter (backbone)                         [10, 3, 520, 520]    [10, 960, 33, 33]    --                   Partial\n",
            "│    └─Conv2dNormActivation (0)                              [10, 3, 520, 520]    [10, 16, 260, 260]   --                   False\n",
            "│    │    └─Conv2d (0)                                       [10, 3, 520, 520]    [10, 16, 260, 260]   (432)                False\n",
            "│    │    └─BatchNorm2d (1)                                  [10, 16, 260, 260]   [10, 16, 260, 260]   (32)                 False\n",
            "│    │    └─Hardswish (2)                                    [10, 16, 260, 260]   [10, 16, 260, 260]   --                   --\n",
            "│    └─InvertedResidual (1)                                  [10, 16, 260, 260]   [10, 16, 260, 260]   --                   False\n",
            "│    │    └─Sequential (block)                               [10, 16, 260, 260]   [10, 16, 260, 260]   (464)                False\n",
            "│    └─InvertedResidual (2)                                  [10, 16, 260, 260]   [10, 24, 130, 130]   --                   False\n",
            "│    │    └─Sequential (block)                               [10, 16, 260, 260]   [10, 24, 130, 130]   (3,440)              False\n",
            "│    └─InvertedResidual (3)                                  [10, 24, 130, 130]   [10, 24, 130, 130]   --                   False\n",
            "│    │    └─Sequential (block)                               [10, 24, 130, 130]   [10, 24, 130, 130]   (4,440)              False\n",
            "│    └─InvertedResidual (4)                                  [10, 24, 130, 130]   [10, 40, 65, 65]     --                   False\n",
            "│    │    └─Sequential (block)                               [10, 24, 130, 130]   [10, 40, 65, 65]     (10,328)             False\n",
            "│    └─InvertedResidual (5)                                  [10, 40, 65, 65]     [10, 40, 65, 65]     --                   False\n",
            "│    │    └─Sequential (block)                               [10, 40, 65, 65]     [10, 40, 65, 65]     (20,992)             False\n",
            "│    └─InvertedResidual (6)                                  [10, 40, 65, 65]     [10, 40, 65, 65]     --                   False\n",
            "│    │    └─Sequential (block)                               [10, 40, 65, 65]     [10, 40, 65, 65]     (20,992)             False\n",
            "│    └─InvertedResidual (7)                                  [10, 40, 65, 65]     [10, 80, 33, 33]     --                   False\n",
            "│    │    └─Sequential (block)                               [10, 40, 65, 65]     [10, 80, 33, 33]     (32,080)             False\n",
            "│    └─InvertedResidual (8)                                  [10, 80, 33, 33]     [10, 80, 33, 33]     --                   False\n",
            "│    │    └─Sequential (block)                               [10, 80, 33, 33]     [10, 80, 33, 33]     (34,760)             False\n",
            "│    └─InvertedResidual (9)                                  [10, 80, 33, 33]     [10, 80, 33, 33]     --                   False\n",
            "│    │    └─Sequential (block)                               [10, 80, 33, 33]     [10, 80, 33, 33]     (31,992)             False\n",
            "│    └─InvertedResidual (10)                                 [10, 80, 33, 33]     [10, 80, 33, 33]     --                   False\n",
            "│    │    └─Sequential (block)                               [10, 80, 33, 33]     [10, 80, 33, 33]     (31,992)             False\n",
            "│    └─InvertedResidual (11)                                 [10, 80, 33, 33]     [10, 112, 33, 33]    --                   False\n",
            "│    │    └─Sequential (block)                               [10, 80, 33, 33]     [10, 112, 33, 33]    (214,424)            False\n",
            "│    └─InvertedResidual (12)                                 [10, 112, 33, 33]    [10, 112, 33, 33]    --                   False\n",
            "│    │    └─Sequential (block)                               [10, 112, 33, 33]    [10, 112, 33, 33]    (386,120)            False\n",
            "│    └─InvertedResidual (13)                                 [10, 112, 33, 33]    [10, 160, 33, 33]    --                   False\n",
            "│    │    └─Sequential (block)                               [10, 112, 33, 33]    [10, 160, 33, 33]    (429,224)            False\n",
            "│    └─InvertedResidual (14)                                 [10, 160, 33, 33]    [10, 160, 33, 33]    --                   True\n",
            "│    │    └─Sequential (block)                               [10, 160, 33, 33]    [10, 160, 33, 33]    797,360              True\n",
            "│    └─InvertedResidual (15)                                 [10, 160, 33, 33]    [10, 160, 33, 33]    --                   True\n",
            "│    │    └─Sequential (block)                               [10, 160, 33, 33]    [10, 160, 33, 33]    797,360              True\n",
            "│    └─Conv2dNormActivation (16)                             [10, 160, 33, 33]    [10, 960, 33, 33]    --                   True\n",
            "│    │    └─Conv2d (0)                                       [10, 160, 33, 33]    [10, 960, 33, 33]    153,600              True\n",
            "│    │    └─BatchNorm2d (1)                                  [10, 960, 33, 33]    [10, 960, 33, 33]    1,920                True\n",
            "│    │    └─Hardswish (2)                                    [10, 960, 33, 33]    [10, 960, 33, 33]    --                   --\n",
            "├─LRASPPHead (classifier)                                    [10, 40, 65, 65]     [10, 32, 65, 65]     --                   True\n",
            "│    └─Sequential (cbr)                                      [10, 960, 33, 33]    [10, 128, 33, 33]    --                   True\n",
            "│    │    └─Conv2d (0)                                       [10, 960, 33, 33]    [10, 128, 33, 33]    122,880              True\n",
            "│    │    └─BatchNorm2d (1)                                  [10, 128, 33, 33]    [10, 128, 33, 33]    256                  True\n",
            "│    │    └─ReLU (2)                                         [10, 128, 33, 33]    [10, 128, 33, 33]    --                   --\n",
            "│    └─Sequential (scale)                                    [10, 960, 33, 33]    [10, 128, 1, 1]      --                   True\n",
            "│    │    └─AdaptiveAvgPool2d (0)                            [10, 960, 33, 33]    [10, 960, 1, 1]      --                   --\n",
            "│    │    └─Conv2d (1)                                       [10, 960, 1, 1]      [10, 128, 1, 1]      122,880              True\n",
            "│    │    └─Sigmoid (2)                                      [10, 128, 1, 1]      [10, 128, 1, 1]      --                   --\n",
            "│    └─Conv2d (low_classifier)                               [10, 40, 65, 65]     [10, 32, 65, 65]     1,312                True\n",
            "│    └─Conv2d (high_classifier)                              [10, 128, 65, 65]    [10, 32, 65, 65]     4,128                True\n",
            "============================================================================================================================================\n",
            "Total params: 3,223,408\n",
            "Trainable params: 2,001,696\n",
            "Non-trainable params: 1,221,712\n",
            "Total mult-adds (G): 20.94\n",
            "============================================================================================================================================\n",
            "Input size (MB): 32.45\n",
            "Forward/backward pass size (MB): 4649.32\n",
            "Params size (MB): 12.89\n",
            "Estimated Total Size (MB): 4694.66\n",
            "============================================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "class Model():\n",
        "\n",
        "    def __init__(self, model_name, optim, loss_type, output_shape=32, verbose=False):\n",
        "        # Initialise class parameters\n",
        "        self.model_name = model_name\n",
        "        self.optimiser_type = optim\n",
        "        self.loss_type = loss_type\n",
        "\n",
        "        self.model_dic = {\n",
        "        'DeepLabV3': [torchvision.models.segmentation.DeepLabV3_ResNet101_Weights.DEFAULT,\n",
        "                        torchvision.models.segmentation.deeplabv3_resnet101],\n",
        "    \n",
        "        'FCN' :      [torchvision.models.segmentation.FCN_ResNet101_Weights.DEFAULT,\n",
        "                        torchvision.models.segmentation.fcn_resnet101],\n",
        "    \n",
        "        'LRASPP' :   [torchvision.models.segmentation.LRASPP_MobileNet_V3_Large_Weights.DEFAULT, \n",
        "                        torchvision.models.segmentation.lraspp_mobilenet_v3_large],\n",
        "\n",
        "        'IoU' :       torchmetrics.classification.MultilabelJaccardIndex(num_labels=output_shape).to(device),\n",
        "\n",
        "        'PixAcc':     torchmetrics.classification.MultilabelAccuracy(num_labels=output_shape).to(device),\n",
        "        }\n",
        "\n",
        "        # Call the createModel function to return the specified model, and initialise accuracy metrics\n",
        "        self.createModel(output_shape, verbose)\n",
        "        self.iou_acc = torchmetrics.classification.MultilabelJaccardIndex(num_labels=output_shape).to(device)\n",
        "        self.pix_acc = torchmetrics.classification.MultilabelAccuracy(num_labels=output_shape).to(device)\n",
        "        \n",
        "    def createModel(self, output_shape, verbose):\n",
        "        \"\"\"Pretrained semantic segmentation model with custom head\n",
        "        Args:\n",
        "            output_shape (int, optional): The number of output channels\n",
        "            in your dataset masks. Defaults to 32.\n",
        "\n",
        "            verbose (bool, optional): Print out the model architecture. \n",
        "            Default is False.\n",
        "\n",
        "        Returns:\n",
        "            model: Returns the desired model with either the ResNet101 (for DeepLabV3 and FCN), \n",
        "            or MobileNet (for LRASPP) backbone.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.weights = self.model_dic[self.model_name][0]\n",
        "        self.model = self.model_dic[self.model_name][1](weights=self.weights)\n",
        "        self.auto_transform = self.weights.transforms()\n",
        "        \n",
        "        # Freeze pretrained \"backbone\" layers\n",
        "        if self.model_name == 'LRASPP':\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if \"backbone\" in name:\n",
        "                    if \"14\" in name or \"15\" in name or \"16\" in name:\n",
        "                        pass     \n",
        "                    else:\n",
        "                        param.requires_grad = False\n",
        "\n",
        "        if self.model_name == 'DeepLabV3' or self.model_name == 'FCN':\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if \"backbone\" in name:\n",
        "                    if \"layer3\" in name or \"layer4\" in name:\n",
        "                        pass     \n",
        "                    else:\n",
        "                        param.requires_grad = False        \n",
        "\n",
        "\n",
        "        # Replace the last classifier layer with a Conv2d layer with the correct output shape\n",
        "        # If the model has an auxiliary classifier, replace the last classifier layer with 32 output channels\n",
        "        \n",
        "        if self.model_name == 'DeepLabV3':\n",
        "            self.model.classifier[-1] = nn.Conv2d(256, output_shape, kernel_size=1, stride=1)\n",
        "            try:\n",
        "                self.model.aux_classifier[-1] = nn.Conv2d(256, output_shape, kernel_size=1, stride=1)\n",
        "                self.model.aux_classifier.add_module('softmax', nn.Softmax(dim=1))\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        if self.model_name == 'FCN':\n",
        "            self.model.classifier[-1] = nn.Conv2d(512, output_shape, kernel_size=1, stride=1)\n",
        "            try:\n",
        "                self.model.aux_classifier[-1] = nn.Conv2d(256, output_shape, kernel_size=1, stride=1)\n",
        "                self.model.aux_classifier.add_module('softmax', nn.Softmax(dim=1))\n",
        "            except:\n",
        "                pass  \n",
        "\n",
        "        if self.model_name == 'LRASPP':\n",
        "            self.model.classifier.high_classifier = nn.Conv2d(128, output_shape, kernel_size=1, stride=1)\n",
        "            self.model.classifier.low_classifier = nn.Conv2d(40, output_shape, kernel_size=1, stride=1)\n",
        "            try:\n",
        "                self.model.aux_classifier[-1] = nn.Conv2d(256, output_shape, kernel_size=1, stride=1)\n",
        "                self.model.aux_classifier.add_module('softmax', nn.Softmax(dim=1))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        #Create optimiser and learning rate scheduler\n",
        "        params = [p for p in self.model.parameters() if p.requires_grad]\n",
        "\n",
        "        if self.model_name == 'DeepLabV3' or self.model_name == 'FCN':\n",
        "            if self.optimiser_type == 'SGD':\n",
        "                self.optimiser = torch.optim.SGD(params, lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
        "            if self.optimiser_type == 'Adam': \n",
        "                self.optimiser = torch.optim.Adam(params, lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001, amsgrad=False)\n",
        "            if self.optimiser_type == 'RMSprop':\n",
        "                self.optimiser = torch.optim.RMSprop(params, lr=0.0001, alpha=0.99, eps=1e-08, weight_decay=0.001, momentum=0.9)\n",
        "            self.lr_scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=4, gamma=0.01) \n",
        "\n",
        "        if self.model_name == 'LRASPP':\n",
        "            if self.optimiser_type == 'SGD':\n",
        "                self.optimiser = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
        "            if self.optimiser_type == 'Adam': \n",
        "                self.optimiser = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001, amsgrad=False)\n",
        "            if self.optimiser_type == 'RMSprop':\n",
        "                self.optimiser = torch.optim.RMSprop(params, lr=0.001, alpha=0.99, eps=1e-08, weight_decay=0.001, momentum=0.9)\n",
        "            self.lr_scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=3, gamma=0.01)\n",
        "\n",
        "        # Initialise either the weighted Cross Entropy Loss or unweighted\n",
        "        if self.loss_type == 'Standard_CEL':\n",
        "            self.loss = nn.CrossEntropyLoss()\n",
        "        if self.loss_type == 'Weighted_CEL':\n",
        "            self.loss = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        \n",
        "        #Optionally print out the new model architecture\n",
        "        if verbose:\n",
        "            print(summary(model=self.model, \n",
        "                input_size=(10, 3, 520, 520),\n",
        "                col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "                col_width=20,\n",
        "                row_settings=[\"var_names\"]\n",
        "            )) \n",
        "\n",
        "        return self.model\n",
        "\n",
        "m1 = Model('DeepLabV3', 'Adam', 'Standard_CEL', verbose=True)\n",
        "m2 = Model('FCN', 'Adam', 'Standard_CEL', verbose=True)\n",
        "m3 = Model('LRASPP', 'Adam', 'Standard_CEL', verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EB7HFiKD0m8X"
      },
      "outputs": [],
      "source": [
        "#Garbage collect and empty cache\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class TrainTest():\n",
        "  def __init__(self, model_class, trainLoader, testLoader, n_epochs=3, batch_size=3, k=10):\n",
        "    \n",
        "    # Initialise model specific attributes\n",
        "    self.model_class = model_class\n",
        "    self.model = self.model_class.model.to(device)\n",
        "    self.lr_scheduler = self.model_class.lr_scheduler\n",
        "    self.optimiser = self.model_class.optimiser\n",
        "    self.loss = self.model_class.loss\n",
        "    self.iou_accuracy = self.model_class.iou_acc\n",
        "    self.pixel_accuracy = self.model_class.pix_acc\n",
        "\n",
        "\n",
        "    # Initialise remaining attributes\n",
        "    self.trainLoader = trainLoader\n",
        "    self.testLoader = testLoader\n",
        "    self.n_epochs = n_epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.k = k\n",
        "\n",
        "    # Check to see whether the combination of batch size and dataloader size will result in a batch size of 1 during training,\n",
        "    # which will break the model. Change the batch size if so.\n",
        "    while len(self.trainLoader)*(self.k - 1)/self.k % self.batch_size == 1:\n",
        "        proceed = input(f\"Invalid batch size. Proceed with batch_size = {self.batch_size + 1}?  [Y/N]\\n\")\n",
        "        if proceed.lower() == 'y':\n",
        "            self.batch_size += 1\n",
        "            break\n",
        "        self.batch_size = input(\"Enter batch size: \")\n",
        "    \n",
        "  def train_epoch(self, dataloader):\n",
        "          self.model.train()\n",
        "          iou_acc = 0.0\n",
        "          pix_acc = 0.0\n",
        "          train_loss = 0.0\n",
        "\n",
        "          scaler = GradScaler()\n",
        "\n",
        "          for idx, (images, labels) in enumerate(dataloader): \n",
        "              images, labels = images.to(device).float(), labels.to(device)\n",
        "\n",
        "              # Calculate output predicitons\n",
        "              self.optimiser.zero_grad()\n",
        "              with autocast(dtype=torch.float16): # Use automatic mixed precision training to optimise memory usage\n",
        "                  outputs = self.model(images)['out']\n",
        "                \n",
        "                  # Convert labels to 32 channel one-hot encoding\n",
        "                  labels = encode_segmap(labels)\n",
        "\n",
        "                  # Calculate loss \n",
        "                  loss = self.loss(outputs, labels.float())\n",
        "                    \n",
        "              # Perform scaled backward pass and update loss and accuracy\n",
        "              scaler.scale(loss).backward()\n",
        "              scaler.step(self.optimiser)\n",
        "              scaler.update()\n",
        "              train_loss += loss.item() * images.size(0)\n",
        "              iou_acc += self.iou_accuracy(outputs, labels).item() * images.size(0)\n",
        "              pix_acc += self.pixel_accuracy(outputs, labels).item() * images.size(0)\n",
        "\n",
        "          # Update learning rate scheduler\n",
        "          self.lr_scheduler.step()\n",
        "\n",
        "          return iou_acc, pix_acc, train_loss\n",
        "      \n",
        "  def validation_epoch(self, dataloader):\n",
        "      \n",
        "      self.model.eval()\n",
        "      validation_iou_acc = 0.0\n",
        "      validation_pix_acc = 0.0\n",
        "      validation_loss = 0.0\n",
        "      \n",
        "      with torch.no_grad():\n",
        "          for idx, (images, labels) in enumerate(dataloader): \n",
        "              images, labels = images.to(device).float(), labels.to(device)\n",
        "    \n",
        "              # Calculate output predicitons\n",
        "              outputs = self.model(images)['out']\n",
        "\n",
        "              # Convert labels to 32 channel one-hot encoding\n",
        "              labels = encode_segmap(labels)\n",
        "\n",
        "              # Calculate accuracy and loss\n",
        "              loss = self.loss(outputs, labels.float())\n",
        "              validation_loss += loss.item() * images.size(0)\n",
        "              validation_iou_acc += self.iou_accuracy(outputs, labels).item() * images.size(0)\n",
        "              validation_pix_acc += self.pixel_accuracy(outputs, labels).item() * images.size(0)\n",
        "\n",
        "      return validation_iou_acc, validation_pix_acc, validation_loss\n",
        "\n",
        "  def k_fold_train(self, shuffle=True, verbose=True):\n",
        "\n",
        "      kfold = KFold(n_splits=self.k, shuffle=shuffle)\n",
        "      cv_average_train_iou_accuracy = []\n",
        "      cv_average_train_pixel_accuracy = []\n",
        "      cv_average_validation_iou_accuracy = []\n",
        "      cv_average_validation_pixel_accuracy = []\n",
        "      cv_average_train_loss = []\n",
        "      cv_average_validation_loss = []\n",
        "      start_time = time.time()\n",
        "\n",
        "      for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(self.trainLoader.dataset)):\n",
        "          print(\"Current Fold:\", fold_idx + 1)\n",
        "\n",
        "          # Create data loaders for the training and validation subsets\n",
        "          train_loader = DataLoader(self.trainLoader, batch_size=self.batch_size, sampler=SubsetRandomSampler(train_idx))\n",
        "          val_loader = DataLoader(self.trainLoader, batch_size=self.batch_size, sampler=SubsetRandomSampler(val_idx))\n",
        "          \n",
        "          # Initialse lists to keep track of accuracy at each epoch\n",
        "          epoch_train_iou_accuracy = []\n",
        "          epoch_train_pixel_accuracy = []\n",
        "          epoch_validation_iou_accuracy = []\n",
        "          epoch_validation_pixel_accuracy = []\n",
        "          epoch_train_loss = []\n",
        "          epoch_validation_loss = []\n",
        "\n",
        "          for epoch in range(self.n_epochs):\n",
        "\n",
        "              train_cumulative_iou_acc, train_cumulative_pix_acc, train_loss = TrainTest.train_epoch(self, train_loader)\n",
        "              validation_cumulative_iou_acc, validation_cumulative_pix_acc, validation_loss = TrainTest.validation_epoch(self, val_loader)\n",
        "              \n",
        "              # Find the IoU and Pixel accuracy, and loss\n",
        "              train_iou_acc = train_cumulative_iou_acc / len(train_loader.sampler) * 100\n",
        "              train_pix_acc = train_cumulative_pix_acc / len(train_loader.sampler) * 100\n",
        "              validation_iou_acc = validation_cumulative_iou_acc / len(val_loader.sampler) * 100\n",
        "              validation_pix_acc = validation_cumulative_pix_acc / len(val_loader.sampler) * 100\n",
        "              train_loss = train_loss / len(train_loader.sampler)\n",
        "              validation_loss = validation_loss / len(val_loader.sampler)\n",
        "\n",
        "              if verbose==True:\n",
        "                  print(\"Epoch:{}/{} Training IoU Acc: {:.2f}%, Validation IoU Acc: {:.2f}%, Training Pixel Acc: {:.2f}%, Validation Pixel Acc: {:.2f}%, Training Loss: {:.3f}, Validation Loss: {:.3f}\".format(\n",
        "                            epoch + 1, self.n_epochs, train_iou_acc, validation_iou_acc, train_pix_acc, validation_pix_acc, train_loss, validation_loss))\n",
        "              \n",
        "              # Append epoch lists keeping track of accuracy and loss at each epoch \n",
        "              epoch_train_iou_accuracy.append(train_iou_acc)\n",
        "              epoch_train_pixel_accuracy.append(train_pix_acc)\n",
        "              epoch_validation_iou_accuracy.append(validation_iou_acc)\n",
        "              epoch_validation_pixel_accuracy.append(validation_pix_acc)\n",
        "              epoch_train_loss.append(train_loss)\n",
        "              epoch_validation_loss.append(validation_loss)\n",
        "          \n",
        "          # Append the CV average lists with the list containing the accuracy and loss for the epochs just gone\n",
        "          cv_average_train_iou_accuracy.append(epoch_train_iou_accuracy)\n",
        "          cv_average_train_pixel_accuracy.append(epoch_train_pixel_accuracy)\n",
        "          cv_average_validation_iou_accuracy.append(epoch_validation_iou_accuracy)\n",
        "          cv_average_validation_pixel_accuracy.append(epoch_validation_pixel_accuracy)\n",
        "          cv_average_train_loss.append(epoch_train_loss)\n",
        "          cv_average_validation_loss.append(epoch_validation_loss)\n",
        "\n",
        "      # Calculate and print the training time\n",
        "      train_time = time.time() - start_time\n",
        "      print(f'The time taken to train the network was {int(train_time // 60)} mins {train_time % 60 :.0f} seconds')\n",
        "\n",
        "      # Find the average accuracy and loss for each epoch (averaging across k folds)\n",
        "      cv_average_train_iou_accuracy = np.mean(cv_average_train_iou_accuracy, axis=0)\n",
        "      cv_average_train_pixel_accuracy = np.mean(cv_average_train_pixel_accuracy, axis=0)\n",
        "      cv_average_validation_iou_accuracy = np.mean(cv_average_validation_iou_accuracy, axis=0)\n",
        "      cv_average_validation_pixel_accuracy = np.mean(cv_average_validation_pixel_accuracy, axis=0)\n",
        "      cv_average_train_loss = np.mean(cv_average_train_loss, axis=0)\n",
        "      cv_average_validation_loss = np.mean(cv_average_validation_loss, axis=0)\n",
        "      \n",
        "      self.cv_average_train_iou_accuracy, self.cv_average_train_pixel_accuracy = cv_average_train_iou_accuracy, cv_average_train_pixel_accuracy\n",
        "      self.cv_average_validation_iou_accuracy, self.cv_average_validation_pixel_accuracy = cv_average_validation_iou_accuracy, cv_average_validation_pixel_accuracy\n",
        "      self.cv_average_train_loss, self.cv_average_validation_loss = cv_average_train_loss, cv_average_validation_loss\n",
        "\n",
        "      return cv_average_train_iou_accuracy, cv_average_train_pixel_accuracy, cv_average_validation_iou_accuracy, cv_average_validation_pixel_accuracy, cv_average_train_loss, cv_average_validation_loss\n",
        "\n",
        "  def test(self):\n",
        "\n",
        "      self.test_loader = DataLoader(self.testLoader, batch_size=self.batch_size, shuffle=True)\n",
        "      \n",
        "      test_iou_acc, test_pix_acc = 0.0, 0.0\n",
        "      test_loss = 0.0\n",
        "      self.model.eval()\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for idx, (images, labels) in enumerate(self.test_loader):\n",
        "              if idx == len(self.testLoader) // self.batch_size:   \n",
        "                  images_copy, labels_copy = images.to(\"cpu\").clone(), labels.to(\"cpu\").clone()\n",
        "              images, labels = images.to(device).float(), labels.to(device)\n",
        "              \n",
        "              # Calculate output predicitons\n",
        "              outputs = self.model(images)['out']\n",
        "  \n",
        "              # Convert labels to 32 channel one-hot encoding\n",
        "              labels = encode_segmap(labels)\n",
        "  \n",
        "              # Calculate accuracy and loss\n",
        "              test_iou_acc += self.iou_accuracy(outputs, labels).item() * images.size(0)\n",
        "              test_pix_acc += self.pixel_accuracy(outputs, labels).item() * images.size(0)\n",
        "              loss = self.loss(outputs, labels.float())\n",
        "              test_loss += loss.item() * images.size(0)\n",
        "      \n",
        "      # Calculate and print the final testing accuracy and loss\n",
        "      test_iou_acc = test_iou_acc / len(self.testLoader) * 100\n",
        "      test_pix_acc = test_pix_acc / len(self.testLoader) * 100\n",
        "      test_loss = test_loss / len(self.testLoader)\n",
        "\n",
        "      print(f\"Final Test IoU Accuracy: {test_iou_acc:.2f}%, Final Test Pixel Accuracy: {test_pix_acc:.2f}%, Final Test Loss: {test_loss:.3f}\")\n",
        "      \n",
        "      # Select random image from last batch and display predicted mask alongside original mask and image\n",
        "      idx = random.randint(0, images.shape[0])\n",
        "      original_image = np.transpose(images_copy[idx], (1, 2, 0))\n",
        "      true_mask = np.transpose(labels_copy[idx], (1, 2, 0))\n",
        "      outputs_copy = self.model(images)['out'].cpu()\n",
        "      outputs_copy = outputs_copy.detach()\n",
        "      predicted_mask = one_hot_to_rgb(outputs_copy[idx])\n",
        "      \n",
        "      plt.figure(figsize=(6,6))\n",
        "      \n",
        "      ax = plt.subplot2grid((2,4),(0,0), colspan=2)\n",
        "      ax.imshow(original_image)\n",
        "      plt.title(\"Original Image\")\n",
        "      plt.axis(\"off\")\n",
        "  \n",
        "      # Convert a one-hot image to RGB and display\n",
        "      ax1 = plt.subplot2grid((2,4),(0,2), colspan=2)\n",
        "      ax1.imshow(true_mask)\n",
        "      plt.title(\"True Mask\")\n",
        "      plt.axis(\"off\")\n",
        "  \n",
        "      ax2 = plt.subplot2grid((2,4),(1,1), colspan=2)\n",
        "      ax2.imshow(predicted_mask)\n",
        "      plt.title(\"Predicted Mask\")\n",
        "      plt.axis(\"off\")\n",
        "  \n",
        "      plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)\n",
        "      plt.show()\n",
        "  \n",
        "  def plot(self):\n",
        "\n",
        "      # Plot graph showing the IoU and Pixel accuracy for the training and validation sets across n epochs\n",
        "      x = [str(i+1) for i in range(self.n_epochs)]\n",
        "      plt.figure()\n",
        "      plt.title(f\"Cross-Validated Average Training Accuracy over {self.n_epochs} epochs\")\n",
        "      plt.plot(x, self.cv_average_train_iou_accuracy, label='Training IoU Accuracy')\n",
        "      plt.plot(x, self.cv_average_train_pixel_accuracy, label='Training Pixel Accuracy')\n",
        "      plt.plot(x, self.cv_average_validation_iou_accuracy, label='Validation IoU Accuracy')\n",
        "      plt.plot(x, self.cv_average_validation_pixel_accuracy, label='Validation Pixel Accuracy')\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Accuracy (%)\")\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "      # Plot graph showing the training and validation loss over n epochs\n",
        "      plt.figure()\n",
        "      plt.title(f\"Cross-Validated Average Training Loss over {self.n_epochs} epochs\")\n",
        "      plt.plot(x, self.cv_average_train_loss, color='red', label='Training Loss')\n",
        "      plt.plot(x, self.cv_average_validation_loss, color='blue', label='Validation Loss')\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.legend()        \n",
        "      plt.show()\n",
        "\n",
        "# run = TrainTest(deepLabV3Model, trainLoader, testLoader, n_epochs=2, batch_size=8, k=3)\n",
        "# run.k_fold_train()\n",
        "# run.test()\n",
        "# run.plot()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsjoKEye99FP"
      },
      "source": [
        "### Hyperparameter tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG06GJk7-H8-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# DeepLab-V3:\n",
        "\n",
        "# deepLabV3Model_SGD_standard_loss = Model('DeepLabV3', 'SGD', 'Standard_CEL')\n",
        "\n",
        "# run = TrainTest(deepLabV3Model_SGD_standard_loss, trainLoader, testLoader, n_epochs=10, batch_size=8, k=3)\n",
        "# run.k_fold_train()\n",
        "\n",
        "# result: \n",
        "\n",
        "# Epoch:10/10 Training Acc: 6.57%, Validation Acc: 6.61%, Training Loss: 3.330, Validation Loss: 3.344\n",
        "# The time taken to train the network was 6 mins 2 seconds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qG_DgRJCRpM",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# deepLabV3Model_Adam_standard_loss = Model('DeepLabV3', 'Adam', 'Standard_CEL')\n",
        "\n",
        "# run = TrainTest(deepLabV3Model_Adam_standard_loss, trainLoader, testLoader, n_epochs=10, batch_size=8, k=3)\n",
        "# run.k_fold_train()\n",
        "\n",
        "# # result: Epoch:10/10 Training Acc: 87.46%, Validation Acc: 87.90%, Training Loss: 1.255, Validation Loss: 1.230\n",
        "# The time taken to train the network was 6 mins 5 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9caUawd1CbK6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# deepLabV3Model_RMSprop_standard_loss = Model('DeepLabV3', 'RMSprop', 'Standard_CEL')\n",
        "\n",
        "# run = TrainTest(deepLabV3Model_RMSprop_standard_loss, trainLoader, testLoader, n_epochs=10, batch_size=8, k=3)\n",
        "# run.k_fold_train()\n",
        "\n",
        "# # result: Epoch:10/10 Training Acc: 36.68%, Validation Acc: 37.01%, Training Loss: 0.400, Validation Loss: 0.355\n",
        "# The time taken to train the network was 6 mins 5 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoOsOIiRFHZ-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# FCN\n",
        "\n",
        "# FCNModel_SGD_Standard_loss = Model('FCN', 'SGD', 'Standard_CEL')\n",
        "# run = TrainTest(FCNModel_SGD_Standard_loss, trainLoader, testLoader, n_epochs=10, batch_size=8, k=3)\n",
        "# run.k_fold_train()\n",
        "\n",
        "# Epoch:10/10 Training Acc: 33.52%, Validation Acc: 35.32%, Training Loss: 3.163, Validation Loss: 3.156\n",
        "# The time taken to train the network was 5 mins 2 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjzmfQ4iFq1N",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# FCN\n",
        "\n",
        "# FCNModel_Adam_Standard_loss = Model('FCN', 'Adam', 'Standard_CEL')\n",
        "# run = TrainTest(FCNModel_Adam_Standard_loss, trainLoader, testLoader, n_epochs=10, batch_size=8, k=3)\n",
        "# run.k_fold_train()\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# Epoch:10/10 Training Acc: 87.31%, Validation Acc: 87.10%, Training Loss: 0.932, Validation Loss: 0.932\n",
        "# The time taken to train the network was 5 mins 28 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw3Ind57Ftjw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# FCN\n",
        "\n",
        "# FCNModel_RMSprop_Standard_loss = Model('FCN', 'RMSprop', 'Standard_CEL')\n",
        "# run = TrainTest(FCNModel_RMSprop_Standard_loss, trainLoader, testLoader, n_epochs=10, batch_size=8, k=3)\n",
        "# run.k_fold_train()\n",
        "\n",
        "# Epoch:10/10 Training Acc: 28.25%, Validation Acc: 28.07%, Training Loss: 0.396, Validation Loss: 0.351\n",
        "# The time taken to train the network was 5 mins 4 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bLHdlbYGStp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# LRASPP\n",
        "\n",
        "# LRASPP_SGD_Standard_loss = Model('LRASPP', 'SGD', 'Standard_CEL')\n",
        "# run = TrainTest(LRASPP_SGD_Standard_loss, trainLoader, testLoader, n_epochs=10, batch_size=8, k=3)\n",
        "# run.k_fold_train()\n",
        "\n",
        "\n",
        "# Epoch:10/10 Training Acc: 26.12%, Validation Acc: 25.46%, Training Loss: 2.037, Validation Loss: 2.070\n",
        "# The time taken to train the network was 1 mins 41 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXcKwEVvGi7Q",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# LRASPP\n",
        "\n",
        "# LRASPP_Adam_Standard_loss = Model('LRASPP', 'Adam', 'Standard_CEL')\n",
        "# run = TrainTest(LRASPP_Adam_Standard_loss, trainLoader, testLoader, n_epochs=10, batch_size=8, k=3)\n",
        "# run.k_fold_train()\n",
        "\n",
        "# Epoch:10/10 Training Acc: 52.67%, Validation Acc: 52.45%, Training Loss: 0.367, Validation Loss: 0.365\n",
        "# The time taken to train the network was 1 mins 42 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ewsw6-TcGoNL",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# LRASPP\n",
        "\n",
        "# LRASPP_RMSprop_Standard_loss = Model('LRASPP', 'RMSprop', 'Standard_CEL')\n",
        "# run = TrainTest(LRASPP_RMSprop_Standard_loss, trainLoader, testLoader, n_epochs=10, batch_size=8, k=3)\n",
        "# run.k_fold_train()\n",
        "\n",
        "# Epoch:10/10 Training Acc: 23.82%, Validation Acc: 23.71%, Training Loss: 0.288, Validation Loss: 0.304\n",
        "# The time taken to train the network was 1 mins 41 seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edk4HoLLeTUG"
      },
      "source": [
        "### Best model testing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7qNnIEceZY1"
      },
      "outputs": [],
      "source": [
        "Best_deepLabV3Model = Model('DeepLabV3', 'Adam', 'Standard_CEL')\n",
        "run = TrainTest(Best_deepLabV3Model, trainLoader, testLoader, n_epochs=5, batch_size=12, k=10)\n",
        "run.k_fold_train()\n",
        "run.test()\n",
        "run.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrnsrppJfqEG"
      },
      "outputs": [],
      "source": [
        "Best_FCNModel = Model('FCN', 'Adam', 'Standard_CEL')\n",
        "run = TrainTest(Best_FCNModel, trainLoader, testLoader, n_epochs=5, batch_size=12, k=10)\n",
        "run.k_fold_train()\n",
        "run.test()\n",
        "run.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wczhBU8TfrjY"
      },
      "outputs": [],
      "source": [
        "Best_LRASPPModel = Model('LRASPP', 'Adam', 'Standard_CEL')\n",
        "run = TrainTest(Best_LRASPPModel, trainLoader, testLoader, n_epochs=10, batch_size=12, k=10)\n",
        "run.k_fold_train()\n",
        "run.test()\n",
        "run.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted Loss Function Model"
      ],
      "metadata": {
        "id": "n6ktHpb4Cbi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LRASPPModel = Model('LRASPP', 'RMSprop', 'Weighted_CEL')\n",
        "run = TrainTest(LRASPPModel, trainLoader, testLoader, n_epochs=5, batch_size=5, k=5)\n",
        "run.k_fold_train()\n",
        "run.test()\n",
        "run.plot()"
      ],
      "metadata": {
        "id": "HjIMhPX98zOr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}